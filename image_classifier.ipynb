{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbNOeWBhkqv1",
        "colab_type": "code",
        "outputId": "3d7dab29-ad47-4374-d052-0e696a943119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "from keras.models import Sequential \n",
        "from keras.layers import Conv2D, MaxPooling2D \n",
        "from keras.layers import Activation, Dropout, Flatten, Dense \n",
        "from keras import backend as K \n",
        "\n",
        "\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "train_data_dir = 'drive/My Drive/MATHS_TRAIN_NEW'\n",
        "validation_data_dir = 'drive/My Drive/MATHS_TEST'\n",
        "nb_train_samples = 230\n",
        "nb_validation_samples = 6\n",
        "epochs = 30\n",
        "batch_size = 15\n",
        "\n",
        "input_shape = (img_width, img_height, 3) #Keras Backend is TensorFlow, so image_data_format is channels_last\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Conv2D(32, (2, 2), input_shape = input_shape)) \n",
        "model.add(Activation('relu')) \n",
        "model.add(MaxPooling2D(pool_size =(2, 2))) \n",
        "\n",
        "model.add(Conv2D(32, (2, 2))) \n",
        "model.add(Activation('relu')) \n",
        "model.add(MaxPooling2D(pool_size =(2, 2))) \n",
        "\n",
        "model.add(Conv2D(64, (2, 2))) \n",
        "model.add(Activation('relu')) \n",
        "model.add(MaxPooling2D(pool_size =(2, 2))) \n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(64)) \n",
        "model.add(Activation('relu')) \n",
        "model.add(Dropout(0.5)) \n",
        "model.add(Dense(6))   # Change me when more categories are added\n",
        "model.add(Activation('sigmoid')) \n",
        "\n",
        "model.compile(loss ='binary_crossentropy', \n",
        "\t\t\t\t\toptimizer ='rmsprop', \n",
        "\t\t\t\tmetrics =['accuracy']) \n",
        "\n",
        "train_datagen = ImageDataGenerator( \n",
        "\t\t\t\trescale = 1. / 255, \n",
        "\t\t\t\tshear_range = 0.2, \n",
        "\t\t\t\tzoom_range = 0.2, \n",
        "\t\t\thorizontal_flip = True) \n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1. / 255) \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, \n",
        "\t\t\t\t\t\t\ttarget_size =(img_width, img_height), \n",
        "\t\t\t\t\tbatch_size = batch_size, class_mode ='categorical') \n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( \n",
        "\t\t\t\t\t\t\t\t\tvalidation_data_dir, \n",
        "\t\t\t\ttarget_size =(img_width, img_height), \n",
        "\t\tbatch_size = batch_size, class_mode ='categorical') \n",
        "\n",
        "model.fit_generator(train_generator, \n",
        "\tsteps_per_epoch = nb_train_samples // batch_size, \n",
        "\tepochs = epochs, validation_data = validation_generator, \n",
        "\tvalidation_steps = nb_validation_samples // batch_size) \n",
        "\n",
        "model.save_weights('model_saved.h5') \n",
        "\n",
        "model.save('model_new.h5') \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Found 230 images belonging to 6 classes.\n",
            "Found 10 images belonging to 6 classes.\n",
            "Epoch 1/30\n",
            "15/15 [==============================] - 12s 792ms/step - loss: 0.6665 - acc: 0.7535 - val_loss: 0.4495 - val_acc: 0.8333\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 11s 761ms/step - loss: 0.5074 - acc: 0.8044 - val_loss: 0.4382 - val_acc: 0.8333\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 11s 736ms/step - loss: 0.4786 - acc: 0.8095 - val_loss: 0.4271 - val_acc: 0.8333\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 11s 746ms/step - loss: 0.4524 - acc: 0.8178 - val_loss: 0.4018 - val_acc: 0.8167\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 11s 721ms/step - loss: 0.4650 - acc: 0.8202 - val_loss: 0.3771 - val_acc: 0.8167\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 11s 758ms/step - loss: 0.4026 - acc: 0.8297 - val_loss: 0.3826 - val_acc: 0.8167\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 12s 803ms/step - loss: 0.3496 - acc: 0.8563 - val_loss: 0.3838 - val_acc: 0.8333\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 11s 766ms/step - loss: 0.3563 - acc: 0.8468 - val_loss: 0.3539 - val_acc: 0.8500\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 11s 734ms/step - loss: 0.2978 - acc: 0.8695 - val_loss: 0.3853 - val_acc: 0.8333\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 11s 738ms/step - loss: 0.3005 - acc: 0.8708 - val_loss: 0.3984 - val_acc: 0.8333\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 11s 733ms/step - loss: 0.3013 - acc: 0.8813 - val_loss: 0.5258 - val_acc: 0.8333\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 12s 777ms/step - loss: 0.2876 - acc: 0.8807 - val_loss: 0.3347 - val_acc: 0.9000\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 11s 752ms/step - loss: 0.3039 - acc: 0.8683 - val_loss: 0.3543 - val_acc: 0.8333\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 11s 753ms/step - loss: 0.2831 - acc: 0.8719 - val_loss: 0.3491 - val_acc: 0.8500\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 11s 751ms/step - loss: 0.3007 - acc: 0.8765 - val_loss: 0.3473 - val_acc: 0.8667\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 11s 756ms/step - loss: 0.2348 - acc: 0.8918 - val_loss: 0.2906 - val_acc: 0.8833\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 11s 759ms/step - loss: 0.2516 - acc: 0.8986 - val_loss: 0.2931 - val_acc: 0.8833\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 12s 784ms/step - loss: 0.2238 - acc: 0.9134 - val_loss: 0.3727 - val_acc: 0.8500\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 12s 770ms/step - loss: 0.2189 - acc: 0.9007 - val_loss: 0.3144 - val_acc: 0.9000\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 11s 762ms/step - loss: 0.2831 - acc: 0.8963 - val_loss: 0.3618 - val_acc: 0.9000\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 11s 724ms/step - loss: 0.1913 - acc: 0.9213 - val_loss: 0.3209 - val_acc: 0.9000\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 11s 735ms/step - loss: 0.2074 - acc: 0.9148 - val_loss: 0.4095 - val_acc: 0.8833\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 11s 752ms/step - loss: 0.1740 - acc: 0.9340 - val_loss: 0.3010 - val_acc: 0.9000\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 12s 791ms/step - loss: 0.1984 - acc: 0.9163 - val_loss: 0.2201 - val_acc: 0.9167\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 11s 726ms/step - loss: 0.2061 - acc: 0.9242 - val_loss: 0.2324 - val_acc: 0.9000\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 11s 736ms/step - loss: 0.2193 - acc: 0.9192 - val_loss: 0.2612 - val_acc: 0.8833\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 12s 785ms/step - loss: 0.1813 - acc: 0.9237 - val_loss: 0.2299 - val_acc: 0.8667\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 11s 720ms/step - loss: 0.1704 - acc: 0.9275 - val_loss: 0.1773 - val_acc: 0.9167\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 11s 729ms/step - loss: 0.1842 - acc: 0.9237 - val_loss: 0.2055 - val_acc: 0.9167\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 11s 723ms/step - loss: 0.1579 - acc: 0.9342 - val_loss: 0.2190 - val_acc: 0.9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gL58b7L4Ejd",
        "colab_type": "code",
        "outputId": "57395652-572d-4670-e5b7-0010f56f6a0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# dimensions of our images\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "\n",
        "\n",
        "# load the model we saved\n",
        "model = load_model('model_new.h5')\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# predicting images\n",
        "img = image.load_img('drive/My Drive/MATHS_TEST/GREEN_RED/41.jpg', target_size=(img_width, img_height))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "images = np.vstack([x])\n",
        "classes = model.predict_classes(images, batch_size=10)\n",
        "\n",
        "\n",
        "# print the classes, the images belong to\n",
        "#print(classes)\n",
        "print(classes[0])\n",
        "\n",
        "available_classes = train_generator.class_indices \n",
        "print(available_classes)\n",
        "\n",
        "\n",
        "print(list(available_classes.keys())[list(available_classes.values()).index(classes[0])])\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "{'BLACK': 0, 'BLACK_GREEN': 1, 'BLACK_RED': 2, 'GREEN': 3, 'GREEN_RED': 4, 'RED': 5}\n",
            "BLACK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty0lIs-EOajY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5c8d11ac-5079-4d67-8555-854869a8c5dd"
      },
      "source": [
        "!cat ~/.keras/keras.json\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"epsilon\": 1e-07, \n",
            "    \"floatx\": \"float32\", \n",
            "    \"image_data_format\": \"channels_last\", \n",
            "    \"backend\": \"tensorflow\"\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BbgWML9GO7Ns"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO_d-fYRP_1x",
        "colab_type": "text"
      },
      "source": [
        "https://keras.io/preprocessing/image/\n",
        "\n",
        "https://www.codesofinterest.com/2017/09/keras-image-data-format.html\n",
        "\n",
        "https://www.geeksforgeeks.org/python-image-classification-using-keras/\n",
        "\n"
      ]
    }
  ]
}